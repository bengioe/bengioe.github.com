<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Theano: a short practical guide</title>
  <!--<script src="https://code.jquery.com/jquery-2.1.4.min.js"></script>  
      <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      <script src="http://d3js.org/d3.v3.min.js" charset="utf-8"></script>-->
  <script src="../zaremba2014/jquery-2.1.4.min.js"></script>
  <script src="../zaremba2014/d3.v3.min.js" charset="utf-8"></script>



  <script src="../show.js"></script>
  <link rel="stylesheet" type="text/css" href="../style.css">

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({TeX : {extensions:["color.js"]}});
  /*MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    TeX : {extensions:["color.js"]},
    jax: ["input/TeX","output/HTML-CSS"],
    tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
  });*/
</script>
<script type="text/javascript" src="../zaremba2014/MathJax-2.4-latest/MathJax.js?config=TeX-AMS_HTML"></script>


</head>
<body>

<div id="pres">


  <section>
    <h1>Theano</h1>
    <h4>A short practical guide</h4><br/><br/>
    Emmanuel Bengio

  </section>

  <section>
    <left>What is Theano?</left><br/>
    <ul>
      <li>A language</li>
      <li>A compiler</li>
      <li>A Python library</li>
    </ul><br/>
    <code>
      <pre><codekw>import</codekw> theano
<codekw>import</codekw> theano.tensor <codekw>as</codekw> T</pre>
    </code><br/>
  </section>

  <section>
    <left>What is Theano?</left><br/>
    What you really do:<br/>
    <ul>
      <li>Build <b>symbolic</b> graphs of computation (w/ input nodes)</li>
      <li>Automatically compute gradients through it<br/> </li>
    </ul><br/>
    <code>
      <pre>gradient = T.grad(cost, parameter)</pre>
    </code><br/>
    <ul>
      <li>Feed some data</li>
      <li>Get results!</li>
    </ul>
    <br/>

  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x", color:"#f00"}],"","",
      [0.8,0.25],
      "xa")
    </script>
    <left>First Example</left><br/>
    <code><pre><codered>x</codered> = T.scalar(<codestr>'x'</codestr>)</pre></code>
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x", color:"#f00"}, {label:"y", color:"#00f"}],
      "","xy",
      [0.8,0.25],
      "xa")
    </script>
    <left>First Example</left><br/>
    <code><pre><codered>x</codered> = T.scalar(<codestr>'x'</codestr>)
<codeblue>y</codeblue> = T.scalar(<codestr>'y'</codestr>)</pre></code>
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x", color:"#f00"}, {label:"y", color:"#00f"}, {label:"z", color:"#0f0"}],
      "xz,yz","xy",
      [0.5,0.65],
      "xa")
    </script>
    <left>First Example</left><br/>
    <code><pre>
<codered>x</codered> = T.scalar(<codestr>'x'</codestr>)
<codeblue>y</codeblue> = T.scalar(<codestr>'y'</codestr>)
<codegrn>z</codegrn> = x + y</pre></code>
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x", color:"#f00"}, {label:"y", color:"#00f"}, {label:"z", color:"#0f0"},
       {label:"add", radius:50}],
      [["x","add"],["y","add"],["add","z"]],"xy",
      [0.5,0.65],
      "xa")
    </script>
    <left>First Example</left><br/>
    <code><pre>
<codered>x</codered> = T.scalar(<codestr>'x'</codestr>)
<codeblue>y</codeblue> = T.scalar(<codestr>'y'</codestr>)
<codegrn>z</codegrn> = x + y</pre></code><br/>
    <icode>'add'</icode> is an <b>Op</b>.
  </section>


  <section>
    <left>Ops in 1 slide</left><br/>
    Ops are the building blocks of the computation graph<br/><br/>
    <div>
    They (usually) define:<br/>
    <ul>
      <li>A computation (given inputs)</li>
      <li>A partial gradient <small>(given inputs and output gradients)</small></li>
      <li>C/CUDA code that does the computation</li>
    </ul>
    </div>

  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x", color:"#f00"}, {label:"y", color:"#00f"}, {label:"z", color:"#0f0"},
       {label:"add", radius:50}],
      [["x","add"],["y","add"],["add","z"]],"xy",
      [0.5,0.65],
      "xa")
    </script>
    <left>First Example</left><br/>
    <code><pre>
<codered>x</codered> = T.scalar()
<codeblue>y</codeblue> = T.scalar()
<codegrn>z</codegrn> = x + y
f = theano.function([x,y],z)
f(2,8) <codecomm># 10</codecomm></pre></code><br/>
  </section>

  <section>
    <left>A 5 line Neural Network <small>(evaluator)</small></left><br/>
    <code><pre>
<codered>x</codered> = T.vector(<codestr>'x'</codestr>)
<codeblue>W</codeblue> = T.matrix(<codestr>'weights'</codestr>)
<codepink>b</codepink> = T.vector(<codestr>'bias'</codestr>)
<codegrn>z</codegrn> = T.nnet.softmax(T.dot(x,W) + b)
f = theano.function([x,W,b],z)</pre></code><br/>
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"a"}, {label:"b"}, {label:"c"},{label:"d"}],
      "ab,bc,cd","ad",
      [0.5,0.7],
      "xa")
    </script>
    <left>A parenthesis about The Graph</left><br/>
    <code><pre>
a = T.vector()
b = f(a)
c = g(b)
d = h(c)
full_fun = theano.function([a],d) <codecomm># h(g(f(a)))</codecomm>
part_fun = theano.function([c],d) <codecomm># h(c)</codecomm></pre></code><br/>
  </section>

  <section>
    <left>Remember the chain rule?</left><br/>
    \(\newcommand{\dd}[2]{\frac{\partial #1}{\partial #2}} \)
    $$ \dd{f}{z} = \dd{f}{a} \dd{a}{z} $$
    $$ \dd{f}{z} = \dd{f}{a} \dd{a}{b} \dd{b}{c} ... \dd{x}{y} \dd{y}{z} $$
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x"}, {label:"2"},{label:"pow", color:"green", radius:50},
       {label:"y"}],
      "x:pow,pow:y,2:pow","2x",
      [0.5,0.7],
      "xa")
    </script>

    <left><icode>T.grad<icode></left><br/>
    <code><pre>x = T.scalar()
y = x ** 2</pre></code></span>
  </section>

  <section onenter="ttt_previous">
    <script>
      DAG(
      [{label:"x"}, {label:"mul", color:"green", radius:50},
       {label:"2"},{label:"pow", color:"green", radius:50},
       {label:"g"}, {label:"y"}],
      "x:pow,pow:y,x:mul,2:mul,mul:g,2:pow","2x",
      [0.5,0.7],
      "xa")
    </script>

    <left><icode>T.grad<icode></left><br/>
    <code><pre>x = T.scalar()
y = x ** 2
g = T.grad(y, x) <codecomm># 2*x</codecomm></pre></code>
  </section>


  <section onenter="ttt_previous">

    <left><icode>T.grad<icode></left><br/>
    $$ \dd{f}{z} = \dd{f}{a} \dd{a}{b} \dd{b}{c} ... \dd{x}{y} \dd{y}{z} $$
    <script>
      DAG(
      [{label:"x"}, {label:"sum", color:"green", radius:50},
       {label:"2"},{label:"pow", color:"green", radius:50},
       {label:"tanh", color:"green", radius:50}, {label:"y"}],
      "x:pow,pow:tanh,tanh:sum,2:pow,sum:y","",
      [0.5,0.7],
      "xa")
    </script>
  </section>

  <section>
    <left><icode>T.grad</icode> take home</left><br/>
    You don't really need to think about the gradient anymore.<br/>
    <ul>
      <li>all you need is a <b>scalar</b> cost</li>
      <li>some parameters</li>
      <li>and a call to <icode>T.grad</icode></li>
    </ul>
  </section>


  <section>
    <left>Shared variables <br/><small>(or, wow, sending things to the GPU is long)</small></left> <br/>
    Data reuse is made through `shared` variables.<br/>
    <code><pre><codered>initial_W</codered> = uniform(-k,k,(n_in, n_out)))
<codepurple>W</codepurple> = theano.shared(value=<codered>initial_W</codered>, name=<codestr>"W"</codestr>)</pre></code><br/>
    That way it sits in the `right` memory spots <br/><small>(e.g. on the GPU if that's where your computation happens)</small>
  </section>

  <section>
    <left>Shared variables</left> <br/>
    Shared variables act like any other node:<br/>
    <code><pre>prediction = T.dot(x,<codepurple>W</codepurple>) + b
cost = T.sum((prediction - target)**2)
gradient = T.<codeblue>grad</codeblue>(cost, <codepurple>W</codepurple>)</pre></code><br/><br/>
    You can compute stuff, take gradients.
  </section>

  <section>
    <left>Shared variables : updating</left> <br/>
    <small>Most importantly, you can:</small> <br/><em>update their value</em>,
    during a function call:<br/>
    <code><pre>
<codeorange>gradient</codeorange> = T.grad(cost, <codepurple>W</codepurple>)
<codeblue>update_list</codeblue> = [(<codepurple>W</codepurple>, <codepurple>W</codepurple> - lr * <codeorange>gradient</codeorange>)]
<codered>f</codered> = theano.function(
       [x,y,lr],[cost],
       <codeblue>updates=update_list</codeblue>)</pre></code><br/><br/>
    Remember, <icode>theano.function</icode> only builds a function. <br/>
    <code><pre><codecomm># this updates W</codecomm>
<codered>f</codered>(minibatch_x, minibatch_y, learning_rate)</pre></code>
  </section>

  <section>
    <left>Shared variables : dataset</left> <br/>
    If dataset is small enough, use a shared variable<br/>
    <code><pre>
index = T.iscalar()
<codeorange>X</codeorange> = theano.shared(data[<codestr>'X'</codestr>])
<codeorange>Y</codeorange> = theano.shared(data[<codestr>'Y'</codestr>])
f = theano.function(
       [index,lr],[cost],
       updates=update_list,
       <codeblue>givens</codeblue>={x:<codeorange>X</codeorange>[index], y:<codeorange>Y</codeorange>[index]})</pre></code><br/><br/>
    You can also take slices:
    <code>X[<codepink>idx</codepink>:<codepink>idx+n</codepink>]</code>
  </section>




  


  <section>
    <left>Printing things</left> <br/>
    There are 3 major ways of printing values:<br/>
    <ol>
      <li>When building the graph</li>
      <li>During execution</li>
      <li>After execution</li>
    </ol><br/>
    <small>And you should do a lot of 1 and 3</small>
  </section>

  <section>
    <left>Printing things when building the graph</left> <br/>
    Use a test value<br/>
    <code><pre>
<codecomm># activate the testing</codecomm>
<codepink>theano.config.compute_test_value</codepink> = <codestr>'raise'</codestr>
x = T.matrix()
x.<codepink>tag.test_value</codepink> = numpy.ones((mbs, n_in))
y = T.vector()
y.<codepink>tag.test_value</codepink> = numpy.ones((mbs,))
    </pre></code><br/>
    <small>You should do this when designing your model to:<br/>
    <ul>
      <li>test shapes</li>
      <li>test types</li>
      <li>...</li>
</ul></small>
  </section>


<!----- Printing ------>

  <section>
    <script>
      DAG(
      [{label:"a"}, {label:"Print", color:"green", radius:50},
       {label:"b"}],
      "a:Print,Print:b","",
      [0.2,0.7],
      "xa")
    </script>

    <left>Printing things when executing a function</left> <br/>
    Use the <icode>Print</icode> Op.<br/>
    <code><pre>
<codekw>from</codekw> theano.printing <codekw>import</codekw> <codepink>Print</codepink>
a = T.nnet.sigmoid(h)
<codecomm># this prints "a:", a.__str__ and a.shape</codecomm>
a = <codepink>Print</codepink>(<codestr>"a"</codestr>,[<codestr>"__str__"</codestr>,<codestr>"shape"</codestr>])(a)
b = something(a)</pre></code><br/>
    <small>
      <div style="padding-left:10em;">
    <ul>
      <li><icode>Print</icode> acts like the identity</li>
      <li>gets activated whenever <codepink>b</codepink> "requests" <codepink>a</codepink></li>
      <li>anything in <icode>dir(numpy.ndarray)</icode> goes</li>
    </ul>
    </div></small>
  </section>


  <section>
    <left>Printing things after execution</left> <br/>
    Add the node to the outputs<br/>
    <code><pre>
theano.function([...],
                [..., <codepink>some_node</codepink>])</pre></code><br/>
    <small>
    Any node can be an ouput <verysmall>(even inputs!)</verysmall><br/>
    You should do this:<br/>
    <ul>
      <li>To aquire statistics</li>
      <li>To monitor gradients, activations...</li>
      <li>With moderation*</li>
    </ul><br/>
    <verysmall>*especially on GPU, as this sends all the data back to the CPU at each call</verysmall>
    </small>
  </section>

  <section>
    <left>Shapes, dimensions, and shuffling</left> <br/>
    You can reshape arrays:<br/>
    <code>b = a.reshape((n,m,p))</code><br/>
    As long as their <em>flat</em> dimension is \(n\times m \times p\)

  </section>

  <section>
    <left>Shapes, dimensions, and shuffling</left> <br/>
    You can change the dimension order:<br/>
    <code><pre>
<codecomm># b[i,k,j] == a[i,j,k]</codecomm>
b = a.dimshuffle(0,2,1)</pre></code><br/>
  </section>

  <section>
    <left>Shapes, dimensions, and shuffling</left> <br/>
    You can also add <b>broadcast dimensions</b>:<br/>
    <code><pre>
<codecomm># a.shape == (n,m)</codecomm>
b = a.dimshuffle(0,<codestr>'x'</codestr>,1)
<codecomm># or </codecomm>
b = a.reshape([n,1,m])</pre></code><br/>
    This allows you to do elemwise* operations <br/> with <icode>b</icode> as if it was \(n\times p \times m\), where <br/>\(p\) can be arbitrary.<br/>
    <small>* e.g. addition, multiplication</small>
  </section>

  <section>
    <left>Broadcasting</left>

    <img src="http://deeplearning.net/software/theano/_images/bcast.png"></img><br/>
    <small> If an array lacks dimensions to match the other operand, the broadcast pattern is automatically expended to the <b>left</b> ( (F,) \(\to\) (T, F), \(\to\) (T, T, F), ...), <br/>  to match the number of dimensions<br/>
    (But you should always do it yourself)</small><br/>

  </section>





<!----- Profiling ------>


  <section>
    <left>Profiling</left><br/>
    When compiling a function, ask theano to profile it:<br/>
    <code>f = theano.function(..., <codepink>profile=True</codepink>)</code><br/><br/>
    when exiting python, it will print the profile.
  </section>


<section>
  <left>Profiling</left>
  <span style="font-size: 0.45em">
  <code><pre>Class
---
<% time> < sum %>< apply time>< time per call>< type><#call>  <#apply> < Class name>
  30.4%    30.4%      10.202s       5.03e-05s     C   202712       4   theano.sandbox.cuda.basic_ops.GpuFromHost
  23.8%    54.2%       7.975s       1.31e-05s     C   608136      12   theano.sandbox.cuda.basic_ops.GpuElemwise
  18.3%    72.5%       6.121s       3.02e-05s     C   202712       4   theano.sandbox.cuda.blas.GpuGemv
   6.0%    78.5%       2.021s       1.99e-05s     C   101356       2   theano.sandbox.cuda.blas.GpuGer
   4.1%    82.6%       1.368s       2.70e-05s     Py   50678       1   theano.tensor.raw_random.RandomFunction
   3.5%    86.1%       1.172s       1.16e-05s     C   101356       2   theano.sandbox.cuda.basic_ops.HostFromGpu
   3.1%    89.1%       1.027s       2.03e-05s     C    50678       1   theano.sandbox.cuda.dnn.GpuDnnSoftmaxGrad
   3.0%    92.2%       1.019s       2.01e-05s     C    50678       1   theano.sandbox.cuda.nnet.GpuSoftmaxWithBias
   2.8%    94.9%       0.938s       1.85e-05s     C    50678       1   theano.sandbox.cuda.basic_ops.GpuCAReduce
   2.4%    97.4%       0.810s       7.99e-06s     C   101356       2   theano.sandbox.cuda.basic_ops.GpuAllocEmpty
   0.8%    98.1%       0.256s       4.21e-07s     C   608136      12   theano.sandbox.cuda.basic_ops.GpuDimShuffle
   0.5%    98.6%       0.161s       3.18e-06s     Py   50678       1   theano.sandbox.cuda.basic_ops.GpuFlatten
   0.5%    99.1%       0.156s       1.03e-06s     C   152034       3   theano.sandbox.cuda.basic_ops.GpuReshape
   0.2%    99.3%       0.075s       4.94e-07s     C   152034       3   theano.tensor.elemwise.Elemwise
   0.2%    99.5%       0.073s       4.83e-07s     C   152034       3   theano.compile.ops.Shape_i
   0.2%    99.7%       0.070s       6.87e-07s     C   101356       2   theano.tensor.opt.MakeVector
   0.1%    99.9%       0.048s       4.72e-07s     C   101356       2   theano.sandbox.cuda.basic_ops.GpuSubtensor
   0.1%   100.0%       0.029s       5.80e-07s     C    50678       1   theano.tensor.basic.Reshape
   0.0%   100.0%       0.015s       1.47e-07s     C   101356       2   theano.sandbox.cuda.basic_ops.GpuContiguous
   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)
</pre></code></span><br/>
  Finding the culprits:<br/>
<code>  24.1%    24.1%       4.537s       1.59e-04s   28611     2   GpuFromHost(x)</code>
</section>

<section>
  <left>Profiling</left>
  A few common names:<br/>
  <span style="font-size: 0.8em">
  <ul>
    <li><b>Gemm/Gemv</b>, matrix\(\times\)matrix / matrix\(\times\)vector</li>
    <li><b>Ger</b>, matrix update</li>
    <li><b>GpuFromHost</b>, data CPU \(\to\) GPU</li>
    <li><b>HostFromGPU</b>, the opposite</li>
    <li><b>[Advanced]Subtensor</b>, indexing</li>
    <li><b>Elemwise</b>, element-per-element Ops (+, -, exp, log, ...)</li>

  </ul>
  </span>
</section>





<!----- SCAN ------>

<section>
  <left>Loops and recurrent models</left>
  Theano has loops, but can be quite complicated.<br/>
  <verysmall>So here's a simple example</verysmall><br/>
  <code><pre>
x = T.vector(<codestr>'x'</codestr>)
n = T.scalar(<codestr>'n'</codestr>)
<codekw>def</codekw> <codeblue>inside_loop</codeblue>(x_t, acc, n):
  <codekw>return</codekw> acc + x_t * n

values, _ = theano.<codepink>scan(</codepink>
     fn = <codeblue>inside_loop</codeblue>,
     sequences=[x],
     outputs_info=[T.zeros(1)],
     non_sequences=[n],
     n_steps=x.shape[0]<codepink>)</codepink>

sum_of_n_times_x = values[<codered>-1</codered>]</pre></code>

</section>

<section>
  <left>Loops and recurrent models</left>
  Line by line:<br/>
  <code><pre>
<codekw>def</codekw> <codeblue>inside_loop</codeblue>(x_t, acc, n):
  <codekw>return</codekw> acc + x_t * n</pre></code>
  <br/>
  <ul>
    <li>This function is called at each iteration</li>
    <li>It takes the arguments in this order:<br/>
    <ol>
      <li>Sequences (default: <icode>seq[t]</icode>)</li>
      <li>Outputs (default: <icode>out[t-1]</icode>)</li>
      <li>Others (no indexing)</li>
    </ol>
    </li>
    <li>It returns <icode>out[t]</icode> for each output</li>
    <li><small>There can be many sequences, many outputs and many others:</li></ul><br/>
      <code>f(seq_0[t], seq_1[t], .., out_0[t-1], out_1[t-1], .., other_0, other_1, ..): <br/></code></small>
</section>


<section>
  <left>Loops and recurrent models</left>
  <code><pre>
values, _ = theano.<codepink>scan(</codepink>
<codecomm># ... </codecomm>
sum_of_n_times_x = values[<codered>-1</codered>]</pre></code><br/>
  <icode>values</icode> is the list/tensor of all outputs through time.<br/>
<small>
  <code><pre>values = [ [out_0[1], out_0[2], ...],
           [out_1[1], out_1[2], ...],
           ...]</pre></code><br/>
  <small>If there's only one output then <icode>values = [out[1], out[2], ...]</icode></small>
</small>
</section>




<section>
  <left>Loops and recurrent models</left>
  <code><pre>
     fn = <codeblue>inside_loop</codeblue>,</pre></code><br/>
  The loop function we saw earlier<br/><br/>
  <code><pre>
     sequences=[x],   </pre></code><br/>
  Sequences are indexed over their <b>first</b> dimension.
</section>




<section>
  <left>Loops and recurrent models</left><br/>
  If you want <icode>out[t-1]</icode> to be an input to the loop function<br/>
  then you need to give <icode>out[0]</icode>.<br/>
  <code><pre>
     outputs_info=[T.zeros(1)], </pre></code><br/><br/>
  If you don't want <icode>out[t-1]</icode> as an input to the loop,<br/>
  pass <icode>None</icode> in outputs_info:<br/>
  <code><pre>
     outputs_info=[None, out_1[0], out_2[0], ...], </pre></code><br/>
  <br/>
  <small>You can also do more advanced "tapping", i.e. get <icode>out[t-k]</icode> </small>
</section>


<section>
  <left>Loops and recurrent models</left><br/>
  <code><pre>
     non_sequences=[n],</pre></code><br/>
  Variables that are used inside the loop (but not indexed).<br/><br/>

  <code><pre>
     n_steps=x.shape[0])</pre></code><br/>
  The number of steps that the loop should do.<br/>
  <small>Note that it is possible to do a "while" loop</small>
</section>



<section>
  <left>Loops and recurrent models</left>
  The whole thing again<br/>
  <code><pre>
x = T.vector(<codestr>'x'</codestr>)
n = T.scalar(<codestr>'n'</codestr>)
<codekw>def</codekw> <codeblue>inside_loop</codeblue>(x_t, acc, n):
  <codekw>return</codekw> acc + x_t * n

values, _ = theano.<codepink>scan(</codepink>
     fn = <codeblue>inside_loop</codeblue>,
     sequences=[x],
     outputs_info=[T.zeros(1)],
     non_sequences=[n],
     n_steps=x.shape[0]<codepink>)</codepink>

sum_of_n_times_x = values[<codered>-1</codered>]</pre></code>

</section>

<section>
  <left>A simple RNN</left><BR/>

  $$ h_t = \mbox{tanh}(x_tW_x + h_{t-1}W_h + b_h) $$
  $$ \hat{y} = \mbox{softmax}(h_{T}W_y + b_y) $$

  <code><pre><codekw>def</codekw> <codeblue>loop</codeblue>(<codegrn>x_t</codegrn>, <codered>h_tm1</codered>, <codepurple>W_x, W_h, b_h</codepurple>):
  <codekw>return</codekw> T.tanh(T.dot(x_t,W_x) +
                T.dot(h_tm1, W_h) +
                b_h)

values,_ = theano.scan(<codeblue>loop</codeblue>,
    [<codegrn>x</codegrn>], [<codered>T.zeros(n_hidden)</codered>], <codepurple>parameters</codepurple>)

y_hat = values[-1]</pre></code>
</section>

<section>
  <left>Dimshuffle and minibatches</left><BR/>
  Usually you want to use minibatches (\(x_{it}\in \mathbb{R}^k\)):<br/>
  <code><pre><codecomm># shape: (batch size, sequence length, k)</codecomm>
x = T.tensor3(<codestr>'x'</codestr>)
<codecomm># define loop ...</codecomm>
v,u = theano.scan(loop,
  [x.dimshuffle(1,0,2)],
  ...)</pre></code><br/>
  This way scan iterates over the "sequence" axis.<br/>
  <small>Otherwise it would iterate over the minibatch examples.</small>
</section>

<section>
  <left>A few tips: make classes</left><BR/>
  Make reusable classes for layers, or parts of your model:<br/>
  <code><pre><codekw>class</codekw> <codeblue>HiddenLayer</codeblue>:
  <codekw>def</codekw> <codeblue>__init__</codeblue>(self, x, n_in, n_hidden):
    self.W = shared(...)
    self.b = shared(...)
    self.output = activation(T.dot(x,W)+b)</pre></code><br/>
   
</section>

<section>
  <left>A few tips: save often</left><BR/>
  It's really easy with theano/python to save and reload data:<br/>
  <code><pre><codekw>class</codekw> <codeblue>HiddenLayer</codeblue>:
  <codekw>def</codekw> <codeblue>__init__</codeblue>(self, x, n_in, n_hidden):
    <codecomm># ...</codecomm>
    self.params = [self.W, self.b]
  <codekw>def</codekw> <codeblue>save_params</codeblue>(self):
     <codekw>return</codekw> [i.get_value() <codekw>for</codekw> i <codekw>in</codekw> self.params]
  <codekw>def</codekw> <codeblue>load_params</codeblue>(self, values):
     <codekw>for</codekw> p, value <codekw>in</codekw> <codepurple>zip</codepurple>(self.params, values):
       p.set_value(value)</pre></code><br/>
   
</section>

<section>
  <left>A few tips: save often</left><BR/>
  It's really easy with theano/python to save and reload data:<br/>
  <code><pre><codekw>import</codekw> cPickle <codekw>as</codekw> pickle
<codecomm># save </codecomm>
pickle.dump(model.save_params(),
            file(<codestr>'model_params.pkl'</codestr>, <codestr>'w'</codestr>)
<codecomm># load </codecomm>
model.load_params(
  pickle.load(
    file(<codestr>'model_params.pkl'</codestr>,<codestr>'r'</codestr>)))</pre></code><br/>
   You can even save whole models and functions with <icode>pickle</icode> but that requires a few additional tricks.
</section>

<section>
  <left>A few tips: error messages</left><br/>
  <small><code><pre>ValueError: GpuElemwise. Input dimension mis-match.  Input 1 (indices
        start at 0) has shape[1] == 256, but the output's size on that axis is 128.
Apply node that caused the error: GpuElemwise{add,no_inplace}
        (&lt;CudaNdarrayType(float32, matrix)>,
         &lt;CudaNdarrayType(float32, matrix)>)
Inputs types: [CudaNdarrayType(float32, matrix),
               CudaNdarrayType(float32, matrix)]
  </pre></code></small><br/>
  It tells us we're trying to multiply \(AB\) but \(A:(n, 128), B:(256, m)\)
</section>

<section>
  <left>A few tips: floatX</left><br/>
  Theano has a default float precision:<br/>
<icode>theano.config.floatX</icode><br/><br/>
For now GPUs can only use float32:<br/>
<small><code style="width:70%">TensorType(float32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to float32, or 2) set "allow_input_downcast=True" when calling "function".</code></small>


</section>

<section>
  <left>A few tips: read the doc</left><br/>
<small><a href="http://deeplearning.net/software/theano/library/tensor/basic.html">http://deeplearning.net/software/theano/library/tensor/basic.html</a></small><br/>
</section>




<!--- the typical mnist tutorial -->

<section>
  <left>MNIST</left><br/>
  <small> <a href="http://deeplearning.net/data/mnist/mnist.pkl.gz">http://deeplearning.net/data/mnist/mnist.pkl.gz</a></small><br/><br/>
  *Opens console*

</section>



<!----- THE END ----->


<section>
  <left>A list of things I haven't talked about<br/><verysmall>(but which you can totally search for)</verysmall></left>
  
  <span style="font-size: 0.8em">
  <ul>
    <li>Random numbers (<icode>T.shared_randomstreams</icode>)</li>
    <li>Convolutions (<icode>T.nnet.conv)</icode></li>
    <li>Printing/Drawing graphs (<icode>theano.printing</icode>)</li>
    <li>Jacobians, Rop, Lop and Hessian-free</li>
    <li>Dealing with NaN/inf</li>
    <li>Extending theano (implementing Ops and types)</li>
    <li>Saving models to files (<icode>pickle</icode>)</li>

  </ul>
  </span>
</section>



<!--
  <section id="fooba2r" onenter="ttt_previous">
    <script>
      DAG(
      [{label:"A"},{label:"B"},{label:"C"},{label:"D"}],
      "AB,BC,AC,AD",
      "",
      [0.75,0.5],
      "A")
    </script>
    This is a test 2;
  </section>
  <section id="fooba2ar" onenter="ttt_previous">
    <script>
      DAG(
      [{label:"A"},{label:"D"}],
      "AD",
      "",
      [0.75,0.5],
      "A")
    </script>
    This is a test 2;
  </section>

<section>
    \(test\left(\frac{x}{y}\right)\)
    $$test\left(\frac{x}{y}\right)$$
    <code>
      <pre>
x = T.vector()
y = T.scalar()
z = x * y
      </pre>
    </code>
  </section>
-->
</div>
<script>build();</script>
</body>
